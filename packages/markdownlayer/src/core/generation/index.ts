import chokidar, { type FSWatcher } from 'chokidar';
import fs from 'fs';
import { slug as githubSlug } from 'github-slugger';
import { globby } from 'globby';
import matter from 'gray-matter';
import type { StaticImageData } from 'next/dist/shared/lib/image-external';
import path from 'path';
import type { ReadTimeResults } from 'reading-time';
import readingTime from 'reading-time';
import type { PackageJson } from 'type-fest';
import { z, type ZodSchema } from 'zod';
import { printNode, zodToTs } from 'zod-to-ts';

import type { BaseDoc, GenerationMode, TocItem } from '@/core/types';
import { version } from '../../../package.json';
import { MarkdownlayerError, MarkdownlayerErrorData, errorMap, getYAMLErrorLine } from '../errors';
import { getFileLastUpdate, type LastUpdateData } from '../git';
import type {
  DocumentDefinition,
  DocumentDefinitionSchema,
  DocumentMeta,
  MarkdownlayerConfig,
  MarkdownlayerConfigPlugins,
} from '../types';
import { bundle, type BundleProps } from './bundle';
import { getConfig } from './config-file';
import type { DataCache } from './data-cache';
import { getFormat } from './format';
import {
  convertDocumentToMjsContent,
  generateTypeName,
  getDataVariableName,
  getDocumentDefinitionGitOptions,
  getDocumentIdAndSlug,
  idToFileName,
  makeVariableName,
  toPascalCase,
} from './utils';

type GeneratedCount = { cached: number; generated: number; total: number };

const autogeneratedNote = `// NOTE This file is auto-generated on build.`;

let configWatcher: FSWatcher | null;
let contentWatcher: FSWatcher | null;

export type GenerateOptions = {
  /**
   * Build mode.
   * Enables production optimizations or development hints.
   */
  mode: GenerationMode;

  /** Current working directory. */
  cwd?: string;

  /** Plugin configuration. */
  pluginConfig?: MarkdownlayerConfig;
};

export async function generate(options: GenerateOptions) {
  const { mode, cwd = process.cwd(), pluginConfig } = options;

  // close the config watcher if it exists
  if (configWatcher) {
    configWatcher.close();
    configWatcher = null;
  }

  // get the config (provided in the plugin or compiled from the config file)
  const outputFolder = path.join(cwd, '.markdownlayer');
  const { configPath, configHash, config } = await getConfig({ cwd, outputFolder, pluginConfig });

  // generate the content (initial)
  await generateInnerWatchIfNecessary({ mode, cwd, outputFolder, config, configHash });

  // watch for config changes in the content folder (development mode only)
  if (mode === 'development' && configPath) {
    const fileName = path.basename(configPath);
    const currentConfigPath = configPath;
    configWatcher = chokidar.watch(configPath, { ignoreInitial: true });
    configWatcher.on('all', async (eventName) => {
      if (eventName === 'add') console.log(`${fileName} added`);
      else if (eventName === 'change') console.log(`${fileName} changed`);
      else if (eventName === 'unlink') {
        console.log(`${fileName} deleted. This requires a restart of the server.`);
        return;
      } else return;

      // get the new config and regenerate the content
      const { configHash, config } = await getConfig({ cwd, outputFolder, pluginConfig, currentConfigPath });
      await generateInnerWatchIfNecessary({ mode, cwd, outputFolder, config, configHash });
    });
  }
}

type GenerateInnerOptions = Pick<GenerateOptions, 'mode'> & {
  cwd: string;
  outputFolder: string;
  config: MarkdownlayerConfig;
  configHash: string;
};

export async function generateInnerWatchIfNecessary(options: GenerateInnerOptions) {
  const { mode, cwd = process.cwd(), outputFolder, config, configHash } = options;

  // close the content watcher if it exists
  if (contentWatcher) {
    contentWatcher.close();
    contentWatcher = null;
  }

  // generate the documents (initial)
  await generateInner({ mode, cwd, outputFolder, config, configHash });

  // watch for content changes in the content folder (development mode only)
  if (mode === 'development') {
    contentWatcher = chokidar.watch(config.contentDirPath, { ignoreInitial: true });
    contentWatcher.on('all', async (eventName, path) => {
      if (eventName === 'add') console.log(`File added: ${path}`);
      else if (eventName === 'change') console.log(`File changed: ${path}`);
      else if (eventName === 'unlink') console.log(`File deleted: ${path}`);
      else return;

      await generateInner({ mode, cwd, outputFolder, config, configHash });
    });
  }
}

async function generateInner(options: GenerateInnerOptions) {
  const {
    mode,
    cwd,
    configHash,
    config: { caching = true, contentDirPath, patterns, definitions, mdAsMarkdoc = false, ...plugins },
  } = options;
  let outputFolder = options.outputFolder;

  // load cache from file if it exists, otherwise create a new cache
  // changes in configuration options and plugins will invalidate the cache
  const cacheFilePath = path.join(outputFolder, `cache/v${version}/data-${configHash}.json`);
  let cache: DataCache = { items: {} };
  if (caching && fs.existsSync(cacheFilePath)) {
    cache = JSON.parse(fs.readFileSync(cacheFilePath, 'utf8'));
  }

  // write package.json
  fs.mkdirSync(outputFolder, { recursive: true });
  await writePackageJson({ outputFolder, configHash });

  // iterate over the definitions and generate the docs
  outputFolder = path.join(outputFolder, 'generated');
  const contentDir = path.join(cwd, contentDirPath);
  const generations: Record<string, GenerationResult> = {};
  for (const [type, def] of Object.entries(definitions)) {
    const generation = await generateDocuments({
      ...def,
      type,
      mode,
      contentDir,
      patterns,
      outputFolder,
      mdAsMarkdoc,
      plugins,
      cache,
    });
    generations[type] = generation;
  }

  // write cache to file
  cache.elapsed = Object.values(cache.items).reduce((acc, item) => acc + item.elapsed, 0);
  fs.mkdirSync(path.dirname(cacheFilePath), { recursive: true });
  fs.writeFileSync(cacheFilePath, JSON.stringify(cache, null, 2), { encoding: 'utf8' });

  // write files that would be imported by the application (index.d.ts, index.mjs)
  await writeRootIndexFiles({ outputFolder, generations });

  // print some stats
  const { cached, total }: GeneratedCount = Object.values(generations)
    .map((g) => g.counts)
    .reduce((acc, count) => {
      acc.cached += count.cached;
      acc.generated += count.generated;
      acc.total += count.total;
      return acc;
    });
  console.log(`Generated ${total} documents (${cached} from cache) in .markdownlayer`);
}

type GenerateDocsOptions = DocumentDefinition & {
  type: string;
  mode: GenerationMode;
  contentDir: string;
  patterns?: string | readonly string[];
  ignoreFiles?: string | readonly string[];
  outputFolder: string;
  mdAsMarkdoc: boolean;
  plugins: MarkdownlayerConfigPlugins;
  cache: DataCache;
};

type GenerationResult = { schema?: DocumentDefinitionSchema; counts: GeneratedCount };
async function generateDocuments(options: GenerateDocsOptions): Promise<GenerationResult> {
  const {
    mode,
    type,
    format = 'detect',
    contentDir,
    patterns = '**/*.{md,mdoc,mdx}',
    git = true,
    readTime = true,
    toc: genToc = false,
    validate,
    ignoreFiles,
    outputFolder,
    mdAsMarkdoc,
    plugins,
    cache,
  } = options;

  // ensure that all definitions have at least one pattern
  if (patterns.length === 0) {
    throw new MarkdownlayerError(MarkdownlayerErrorData.ConfigNoPatternsError);
  }

  // get the git options for the document definition
  const { updated: gitUpdatedEnabled, authors: gitAuthorsEnabled } = getDocumentDefinitionGitOptions(git);

  // find the files
  const definitionDir = path.join(contentDir, type);
  const files = await globby(patterns, { cwd: definitionDir, ignoreFiles: ignoreFiles, gitignore: true });

  let cached = 0;
  let generated = 0;
  const docs: BaseDoc[] = [];
  let collectionChanged = false;

  fs.mkdirSync(path.join(outputFolder, type), { recursive: true });

  let schema = options.schema;
  if (typeof schema === 'function') {
    schema = schema({
      // TODO: figure out how to handle images in the schema without needing to pass the file path (module augmentation may help)
      image: (optional) =>
        // @ts-expect-error - The type is correct but the error is due to the transform function
        (optional ? z.string().optional() : z.string()).transform(
          (): StaticImageData => ({ src: '', height: 0, width: 0 }),
        ),
      // image: (optional) => createImage({ optional, shouldEmitFile: false, mode, sourceFilePath }),
    });
  }

  // parse the files and "compile" in a loop
  for (const file of files) {
    const sourceFilePath = path.join(definitionDir, file);

    // if the file has not been modified, use the cached version
    const hash = fs.statSync(sourceFilePath).mtimeMs.toString();
    const cacheEntry = cache.items[file];
    const changed = !cacheEntry || cacheEntry.hash !== hash;
    if (!changed) {
      docs.push(cacheEntry.document);
      cached++;
      continue;
    }

    // at this point we know that the file has changed and we need to recompile
    // it also means that the collection has changed
    collectionChanged = true;

    const start = performance.now();

    const contents = fs.readFileSync(sourceFilePath, 'utf8');
    const parsedMatter = matter(contents);
    const frontmatter = parsedMatter.data as Record<string, unknown>;

    // determine the document format
    let documentFormat = getFormat({ file, format });
    if (documentFormat === 'md' && mdAsMarkdoc) documentFormat = 'mdoc';

    const bundleOptions: BundleProps = {
      contents,
      entryPath: sourceFilePath,
      format: documentFormat,
      mode,
      plugins,
      frontmatter,
    };
    const { code, errors } = await bundle(bundleOptions);
    if (errors && errors.length) {
      console.error(errors);
      throw new Error('Failed to bundle file: ' + file);
    }

    const end = performance.now();
    const elapsed = end - start;

    const { id, slug } = getDocumentIdAndSlug(file);

    //  only pull git info if enabled
    let lastUpdate: LastUpdateData | null = null;
    if (gitUpdatedEnabled || gitUpdatedEnabled) {
      // in production mode use git, otherwise set default values
      if (mode === 'production') {
        lastUpdate = await getFileLastUpdate(path.join(definitionDir, file));
      }
      lastUpdate ??= { date: new Date(), timestamp: 0, author: 'unknown' };
    }

    // only calculate read time if enabled
    let readTimeResults: ReadTimeResults | undefined;
    if (readTime) {
      readTimeResults = readingTime(contents);
    }

    const meta: DocumentMeta = {
      _id: id,
      _filePath: sourceFilePath,

      type: type,
      format: documentFormat,
      body: { raw: contents, code: code },
      frontmatter: frontmatter,
      slug: (frontmatter.slug as string) ?? slug,
      git: lastUpdate == undefined ? undefined : { date: lastUpdate.date, authors: [lastUpdate.author] },
      readTime: readTimeResults,
    };

    let data: Record<string, unknown> = frontmatter;
    if (schema) {
      // Note: will not work for `z.union` or `z.intersection` schemas
      if (typeof schema === 'object' && 'shape' in schema) {
        // Catch reserved `slug` field inside content schemas
        if (schema.shape.slug) {
          throw new MarkdownlayerError({
            ...MarkdownlayerErrorData.DefinitionSchemaContainsSlugError,
            message: MarkdownlayerErrorData.DefinitionSchemaContainsSlugError.message({ definition: type }),
          });
        }

        // we set updated in the frontmatter if:
        // - it is present in the git metadata
        // - functionality is enabled
        // - it is not already set
        if (meta.git?.date && gitUpdatedEnabled) {
          if (!frontmatter.updated) {
            // we set the string and let the schema handle the coercion if necessary
            frontmatter.updated = meta.git.date.toISOString();
          }
        }

        // we set authors/author in the frontmatter if:
        // - it is present in git metadata
        // - functionality is enabled
        // - it is not already present
        if (meta.git?.authors && gitAuthorsEnabled) {
          if (!frontmatter.authors) {
            frontmatter.authors = meta.git.authors;
          }

          // first author is the latest
          if (!frontmatter.author && meta.git.authors.length > 0) {
            frontmatter.author = meta.git.authors[0];
          }
        }
      }

      // Use `safeParseAsync` to allow async transforms
      let formattedError;
      const parsed = await (schema as ZodSchema).safeParseAsync(frontmatter, {
        errorMap(error, ctx) {
          if (error.code === 'custom' && error.params?.isHoistedMarkdownlayerError) {
            formattedError = error.params?.markdownlayerError;
          }
          return errorMap(error, ctx);
        },
      });
      if (parsed.success) {
        data = parsed.data as Record<string, unknown>;
      } else {
        if (!formattedError) {
          formattedError = new MarkdownlayerError({
            ...MarkdownlayerErrorData.InvalidDocumentFrontmatterError,
            message: MarkdownlayerErrorData.InvalidDocumentFrontmatterError.message({
              definition: type,
              documentId: meta._id,
              error: parsed.error,
            }),
            location: {
              file: sourceFilePath,
              line: getYAMLErrorLine(parsedMatter.matter, String(parsed.error.errors[0].path[0])),
              column: 0,
            },
          });
        }
        throw formattedError;
      }
    }

    // generate table of contents if requested
    const toc = frontmatter.toc ?? genToc ? generateToc(contents) : undefined;

    const document: BaseDoc & { data: Record<string, unknown> } = {
      ...meta,

      tableOfContents: toc,

      data,
    };

    // validate the document then add it to the list
    if (validate) await validate(document);
    docs.push(document);

    // write mjs file
    const outputFilePath = path.join(outputFolder, type, `${idToFileName(document._id)}.mjs`);
    const lines = [autogeneratedNote, '', convertDocumentToMjsContent(document)];
    fs.writeFileSync(outputFilePath, lines.join('\n'), { encoding: 'utf8' });

    // update the cache
    cache.items[file] = { hash, type, document, elapsed };
    generated++;
  }

  // ensure that all documents have a unique slug
  const slugs = docs.map((doc) => doc.slug);
  const uniqueSlugs = new Set(slugs);
  if (slugs.length !== uniqueSlugs.size) {
    const duplicateSlugs = slugs.filter((slug, index) => slugs.indexOf(slug) !== index);
    const documentsWithDuplicateSlugs = docs.filter((doc) => duplicateSlugs.includes(doc.slug)).map((doc) => doc._id);
    throw new MarkdownlayerError({
      ...MarkdownlayerErrorData.DuplicateDocumentSlugError,
      message: MarkdownlayerErrorData.DuplicateDocumentSlugError.message({
        definition: type,
        slugs: duplicateSlugs,
        documents: documentsWithDuplicateSlugs,
      }),
      hint: `Remove slug from the frontmatter of the documents and ensure only one file per extension/format.`,
    });
  }

  // write the collection files if there are collection changes (or if there are no documents, to allow imports)
  if (collectionChanged || docs.length == 0) {
    // write import file
    const outputFilePath = path.join(outputFolder, type, 'index.mjs');
    const lines: string[] = [
      autogeneratedNote,
      '',
      ...docs.map((doc) => `import ${makeVariableName(doc._id)} from './${idToFileName(doc._id)}.mjs';`),
      '',
      `export const ${getDataVariableName(toPascalCase(type))} = [${docs.map((doc) => `${makeVariableName(doc._id)}`).join(', ')}]`,
      '',
    ];
    fs.writeFileSync(outputFilePath, lines.join('\n'), { encoding: 'utf8' });
  }

  return { schema, counts: { cached, generated, total: docs.length } };
}

type WritePackageJsonFileOptions = { outputFolder: string; configHash: string };

async function writePackageJson({ outputFolder, configHash }: WritePackageJsonFileOptions) {
  const packageJson: PackageJson & { typesVersions: Record<string, unknown> } = {
    name: 'dot-markdownlayer',
    description: 'This package is auto-generated by markdownlayer.',
    version: `${version}-${configHash}`,
    private: true,
    exports: {
      './generated': {
        import: './generated/index.mjs',
      },
    },
    typesVersions: {
      '*': {
        generated: ['./generated'],
      },
    },
  };

  const filePath = path.join(outputFolder, 'package.json');
  fs.writeFileSync(filePath, JSON.stringify(packageJson, null, 2), { encoding: 'utf8' });
}

type WriteRootIndexFilesOptions = { outputFolder: string; generations: Record<string, GenerationResult> };
async function writeRootIndexFiles({ outputFolder, generations }: WriteRootIndexFilesOptions) {
  const definitionTypes = Object.keys(generations);

  // write the index.d.ts file
  let lines: string[] = [
    autogeneratedNote,
    '',
    `import type { ImageData, BaseDoc } from 'markdownlayer/core';`,
    '',
    `export type { ImageData };`,
    '',
    ...Object.entries(generations).map(([type, { schema }]) => {
      const converted = (schema ? printNode(zodToTs(schema).node) : undefined)?.replace(';\n}', ';\n  }');
      return `export type ${generateTypeName(type)} = BaseDoc & {\n  data: ${converted ?? 'any'};\n};\n`;
    }),
    '',
    `export type DocumentTypes = ${definitionTypes.map((t) => generateTypeName(t)).join(` | `)}`,
    `export type DocumentTypeNames = '${definitionTypes.join(`' | '`)}'`,
    '',
    '',
    ...definitionTypes.map(
      (type) => `export declare const ${getDataVariableName(toPascalCase(type))}: ${generateTypeName(type)}[];`,
    ),
    '',
    `export declare const allDocumentTypes: DocumentTypes[]`,
    '',
  ];
  let filePath = path.join(outputFolder, 'index.d.ts');
  fs.writeFileSync(filePath, lines.join('\n'), { encoding: 'utf8' });

  // write the index.mjs file
  lines = [
    autogeneratedNote,
    '',
    ...definitionTypes.map(
      (type) => `import { ${getDataVariableName(toPascalCase(type))} } from './${type}/index.mjs';`,
    ),
    '',
    `export { ${definitionTypes.map((type) => getDataVariableName(toPascalCase(type))).join(', ')} };`,
    '',
    `export const allDocuments = [...${definitionTypes.map((type) => getDataVariableName(toPascalCase(type))).join(', ...')}];`,
    '',
  ];
  filePath = path.join(outputFolder, 'index.mjs');
  fs.writeFileSync(filePath, lines.join('\n'), { encoding: 'utf8' });
}

// For some reason, generating toc using mdast-util-toc or even using docusaurus' own toc plugin generates entries from frontmatter.
// Most of the time it is the first entry but sometimes it is not. Removing it results in missing TOC at times.
// So we have to do it manually using regex. Hopefully this is a temporary solution and someone will fix this someday.
// Regex solution from https://yusuf.fyi/posts/contentlayer-table-of-contents

const regXHeader = /\n(?<flag>#{1,6})\s+(?<content>.+)/g;

export function generateToc(contents: string): TocItem[] {
  return Array.from(contents.matchAll(regXHeader)).map(({ groups }) => {
    const { flag, content } = groups!;
    // Note: using `slug` instead of `new Slugger()` means no slug deduping.
    const id = githubSlug(content);
    return {
      level: flag.length,
      value: content,
      id: id,
      url: `#${id}`,
    };
  });
}
