import chokidar from 'chokidar';
import fs from 'fs';
import { slug as githubSlug } from 'github-slugger';
import { globby } from 'globby';
import matter from 'gray-matter';
import type { StaticImageData } from 'next/dist/shared/lib/image-external';
import path from 'path';
import type { ReadTimeResults } from 'reading-time';
import readingTime from 'reading-time';
import { z, type ZodSchema } from 'zod';

import type { BaseDoc, GenerationMode, TocItem } from '@/core/types';
import { version } from '../../../package.json';
import { MarkdownlayerError, MarkdownlayerErrorData, errorMap, getYAMLErrorLine } from '../errors';
import { getFileLastUpdate, type LastUpdateData } from '../git';
import type { DocumentDefinition, DocumentDefinitionSchema, DocumentMeta, MarkdownlayerConfigPlugins } from '../types';
import { bundle, type BundleProps } from './bundle';
import { getConfig, type ResolvedConfig } from './config-file';
import type { DataCache } from './data-cache';
import { getFormat } from './format';
import {
  convertDocumentToMjsContent,
  generateTypeName,
  getDataVariableName,
  getDocumentDefinitionGitOptions,
  getDocumentIdAndSlug,
  idToFileName,
  makeVariableName,
} from './utils';

type GeneratedCount = { cached: number; generated: number; total: number };

const autogeneratedNote = `// NOTE This file is auto-generated on build.`;

export type GenerateOptions = {
  /**
   * Build mode.
   * Enables production optimizations or development hints.
   */
  mode: GenerationMode;

  /**
   * The path to the configuration file.
   */
  configPath?: string;
};

export async function generate({ mode, configPath: providedConfigPath }: GenerateOptions) {
  // ensure we have known modes
  if (mode !== 'development' && mode !== 'production') {
    throw new MarkdownlayerError({
      ...MarkdownlayerErrorData.InvalidGenerationModeError,
      message: MarkdownlayerErrorData.InvalidGenerationModeError.message({ mode }),
    });
  }

  // get the config
  const { configPath, configImports, ...config } = await getConfig({ configPath: providedConfigPath });

  // generate the content (initial)
  const cwd = process.cwd();
  const outputFolder = path.join(cwd, '.markdownlayer');
  await generateInner({ mode, cwd, outputFolder, configPath, ...config });

  // watch for changes in the config or content folder (development mode only)
  if (mode === 'development' && configPath) {
    const files = [config.contentDirPath];
    files.push(...configImports); // watch config file and its dependencies

    const watcher = chokidar.watch(files, {
      cwd,
      ignored: /(^|[/\\])[._]./, // ignore dot & underscore files
      ignoreInitial: true, // ignore initial scan
    });
    watcher.on('all', async (eventName, fileName) => {
      if (eventName === 'addDir' || eventName === 'unlinkDir') return; // ignore dir changes
      if (eventName === 'add') console.log(`${fileName} added`);
      else if (eventName === 'change') console.log(`${fileName} changed`);
      else if (eventName === 'unlink') console.log(`${fileName} deleted.`);

      // changes in the config file should restart the whole process
      if (configImports.includes(fileName)) {
        console.log('markdownlayer config changed, restarting...');
        watcher?.close();
        return generate({ mode, configPath: providedConfigPath });
      }

      // regenerate the content
      await generateInner({ mode, cwd, outputFolder, configPath, ...config });
    });
  }
}

type GenerateInnerOptions = {
  mode: GenerationMode;
  cwd: string;
  outputFolder: string;
} & Omit<ResolvedConfig, 'configImports'>;

async function generateInner(options: GenerateInnerOptions) {
  const {
    mode,
    cwd,
    configPath,
    configHash,

    caching = true,
    contentDirPath,
    patterns,
    definitions,
    mdAsMarkdoc = false,
    ...plugins
  } = options;
  let outputFolder = options.outputFolder;

  // load cache from file if it exists, otherwise create a new cache
  // changes in mode, version, configuration options, plugins will invalidate the cache
  const cacheFilePath = path.join(outputFolder, `cache/${mode}/v${version}/data-${configHash}.json`);
  let cache: DataCache = { items: {} };
  if (caching && fs.existsSync(cacheFilePath)) {
    cache = JSON.parse(fs.readFileSync(cacheFilePath, 'utf8'));
  }

  // iterate over the definitions and generate the docs
  outputFolder = path.join(outputFolder, 'generated');
  const contentDir = path.join(cwd, contentDirPath);
  const generations: Record<string, GenerationResult> = {};
  for (const [type, def] of Object.entries(definitions)) {
    const generation = await generateDocuments({
      ...def,
      type,
      mode,
      contentDir,
      patterns,
      outputFolder,
      mdAsMarkdoc,
      plugins,
      cache,
    });
    generations[type] = generation;
  }

  // write cache to file
  cache.elapsed = Object.values(cache.items).reduce((acc, item) => acc + item.elapsed, 0);
  fs.mkdirSync(path.dirname(cacheFilePath), { recursive: true });
  fs.writeFileSync(cacheFilePath, JSON.stringify(cache, null, 2), { encoding: 'utf8' });

  // write files that would be imported by the application (index.d.ts, index.mjs)
  await writeRootIndexFiles({ outputFolder, configPath, generations });

  // print some stats
  const { cached, total }: GeneratedCount = Object.values(generations)
    .map((g) => g.counts)
    .reduce((acc, count) => {
      acc.cached += count.cached;
      acc.generated += count.generated;
      acc.total += count.total;
      return acc;
    });
  console.log(`Generated ${total} documents (${cached} from cache) in .markdownlayer`);
}

type GenerateDocsOptions = DocumentDefinition & {
  type: string;
  mode: GenerationMode;
  contentDir: string;
  patterns?: string | readonly string[];
  outputFolder: string;
  mdAsMarkdoc: boolean;
  plugins: MarkdownlayerConfigPlugins;
  cache: DataCache;
};

type GenerationResult = { schema?: DocumentDefinitionSchema; counts: GeneratedCount };
async function generateDocuments(options: GenerateDocsOptions): Promise<GenerationResult> {
  const {
    mode,
    type,
    format = 'detect',
    contentDir,
    patterns = '**/*.{md,mdoc,mdx}',
    git = true,
    readTime = true,
    toc: genToc = false,
    validate,
    outputFolder,
    mdAsMarkdoc,
    plugins,
    cache,
  } = options;

  // ensure that all definitions have at least one pattern
  if (patterns.length === 0) {
    throw new MarkdownlayerError(MarkdownlayerErrorData.ConfigNoPatternsError);
  }

  // get the git options for the document definition
  const { updated: gitUpdatedEnabled, authors: gitAuthorsEnabled } = getDocumentDefinitionGitOptions(git);

  // find the files
  const definitionDir = path.join(contentDir, type);
  const files = await globby(patterns, {
    cwd: definitionDir,
    gitignore: true, // use .gitignore
    ignore: ['**/_*'], // ignore files starting with underscore
    dot: false, // ignore dot files
    onlyFiles: true, // only files, skip directories
  });

  let cached = 0;
  let generated = 0;
  const docs: BaseDoc[] = [];
  let collectionChanged = false;

  fs.mkdirSync(path.join(outputFolder, type), { recursive: true });

  let schema = options.schema;
  if (typeof schema === 'function') {
    schema = schema({
      // TODO: figure out how to handle images in the schema without needing to pass the file path (module augmentation may help)
      image: (options) =>
        // @ts-expect-error - The type is correct but the error is due to the transform function
        (options?.optional ? z.string({ ...options }).optional() : z.string({ ...options })).transform(
          (): StaticImageData => ({ src: '', height: 0, width: 0 }),
        ),
      // image: (options) => createImage({ ...options, shouldEmitFile: false, mode, sourceFilePath }),
    });
  }

  // parse the files and "compile" in a loop
  for (const file of files) {
    const sourceFilePath = path.normalize(path.join(definitionDir, file));

    // using the sourceFilePath as the key ensure no collisions for files named the same but in different directories;
    const cacheEntryKey = sourceFilePath;

    // if the file has not been modified, use the cached version
    const hash = fs.statSync(sourceFilePath).mtimeMs.toString();
    const cacheEntry = cache.items[cacheEntryKey];
    const changed = !cacheEntry || cacheEntry.hash !== hash;
    if (!changed) {
      docs.push(cacheEntry.document);
      cached++;
      continue;
    }

    // at this point we know that the file has changed and we need to recompile
    // it also means that the collection has changed
    collectionChanged = true;

    const start = performance.now();

    const contents = fs.readFileSync(sourceFilePath, 'utf8');
    const parsedMatter = matter(contents);
    const frontmatter = parsedMatter.data as Record<string, unknown>;

    // determine the document format
    let documentFormat = getFormat({ file: sourceFilePath, format });
    if (documentFormat === 'md' && mdAsMarkdoc) documentFormat = 'mdoc';

    const bundleOptions: BundleProps = {
      contents,
      entryPath: sourceFilePath,
      format: documentFormat,
      mode,
      plugins,
      frontmatter,
    };
    const { code, errors } = await bundle(bundleOptions);
    if (errors && errors.length) {
      console.error(errors);
      throw new Error(`Failed to bundle file: ${sourceFilePath}`);
    }

    const end = performance.now();
    const elapsed = end - start;

    const { id, slug } = getDocumentIdAndSlug(file);

    //  only pull git info if enabled
    let lastUpdate: LastUpdateData | null = null;
    if (gitUpdatedEnabled || gitUpdatedEnabled) {
      // in production mode use git, otherwise set default values
      if (mode === 'production') {
        lastUpdate = await getFileLastUpdate(sourceFilePath);
      }
      lastUpdate ??= { date: new Date(), timestamp: 0, author: 'unknown' };
    }

    // only calculate read time if enabled
    let readTimeResults: ReadTimeResults | undefined;
    if (readTime) {
      readTimeResults = readingTime(contents);
    }

    const meta: DocumentMeta = {
      _id: id,
      _filePath: sourceFilePath,

      type: type,
      format: documentFormat,
      body: { raw: contents, code: code },
      frontmatter: frontmatter,
      slug: (frontmatter.slug as string) ?? slug,
      git: lastUpdate == undefined ? undefined : { date: lastUpdate.date, authors: [lastUpdate.author] },
      readTime: readTimeResults,
    };

    let data: Record<string, unknown> = frontmatter;
    if (schema) {
      // Note: will not work for `z.union` or `z.intersection` schemas
      if (typeof schema === 'object' && 'shape' in schema) {
        // Catch reserved `slug` field inside content schemas
        if (schema.shape.slug) {
          throw new MarkdownlayerError({
            ...MarkdownlayerErrorData.DefinitionSchemaContainsSlugError,
            message: MarkdownlayerErrorData.DefinitionSchemaContainsSlugError.message({ definition: type }),
          });
        }

        // we set updated in the frontmatter if:
        // - it is present in the git metadata
        // - functionality is enabled
        // - it is not already set
        if (meta.git?.date && gitUpdatedEnabled) {
          if (!frontmatter.updated) {
            // we set the string and let the schema handle the coercion if necessary
            frontmatter.updated = meta.git.date.toISOString();
          }
        }

        // we set authors/author in the frontmatter if:
        // - it is present in git metadata
        // - functionality is enabled
        // - it is not already present
        if (meta.git?.authors && gitAuthorsEnabled) {
          if (!frontmatter.authors) {
            frontmatter.authors = meta.git.authors;
          }

          // first author is the latest
          if (!frontmatter.author && meta.git.authors.length > 0) {
            frontmatter.author = meta.git.authors[0];
          }
        }
      }

      // Use `safeParseAsync` to allow async transforms
      let formattedError;
      const parsed = await (schema as ZodSchema).safeParseAsync(frontmatter, {
        errorMap(error, ctx) {
          if (error.code === 'custom' && error.params?.isHoistedMarkdownlayerError) {
            formattedError = error.params?.markdownlayerError;
          }
          return errorMap(error, ctx);
        },
      });
      if (parsed.success) {
        data = parsed.data as Record<string, unknown>;
      } else {
        if (!formattedError) {
          formattedError = new MarkdownlayerError({
            ...MarkdownlayerErrorData.InvalidDocumentFrontmatterError,
            message: MarkdownlayerErrorData.InvalidDocumentFrontmatterError.message({
              definition: type,
              documentId: meta._id,
              error: parsed.error,
            }),
            location: {
              file: sourceFilePath,
              line: getYAMLErrorLine(parsedMatter.matter, String(parsed.error.errors[0].path[0])),
              column: 0,
            },
          });
        }
        throw formattedError;
      }
    }

    // generate table of contents if requested
    const toc = frontmatter.toc ?? genToc ? generateToc(contents) : undefined;

    const document: BaseDoc & { data: Record<string, unknown> } = {
      ...meta,

      tableOfContents: toc,

      data,
    };

    // validate the document then add it to the list
    if (validate) await validate(document);
    docs.push(document);

    // write mjs file
    const outputFilePath = path.join(outputFolder, type, `${idToFileName(document._id)}.mjs`);
    const lines = [autogeneratedNote, '', convertDocumentToMjsContent(document)];
    fs.writeFileSync(outputFilePath, lines.join('\n'), { encoding: 'utf8' });

    // update the cache
    cache.items[cacheEntryKey] = { hash, type, document, elapsed };
    generated++;
  }

  // ensure that all documents have a unique slug
  const slugs = docs.map((doc) => doc.slug);
  const uniqueSlugs = new Set(slugs);
  if (slugs.length !== uniqueSlugs.size) {
    const duplicateSlugs = slugs.filter((slug, index) => slugs.indexOf(slug) !== index);
    const documentsWithDuplicateSlugs = docs.filter((doc) => duplicateSlugs.includes(doc.slug)).map((doc) => doc._id);
    throw new MarkdownlayerError({
      ...MarkdownlayerErrorData.DuplicateDocumentSlugError,
      message: MarkdownlayerErrorData.DuplicateDocumentSlugError.message({
        definition: type,
        slugs: duplicateSlugs,
        documents: documentsWithDuplicateSlugs,
      }),
      hint: `Remove slug from the frontmatter of the documents and ensure only one file per extension/format.`,
    });
  }

  // write the collection files if there are collection changes (or if there are no documents, to allow imports)
  if (collectionChanged || docs.length == 0) {
    // write import file
    const outputFilePath = path.join(outputFolder, type, 'index.mjs');
    const lines: string[] = [
      autogeneratedNote,
      '',
      ...docs.map((doc) => `import ${makeVariableName(doc._id)} from './${idToFileName(doc._id)}.mjs';`),
      '',
      `export const ${getDataVariableName(type)} = [${docs.map((doc) => `${makeVariableName(doc._id)}`).join(', ')}]`,
      '',
    ];
    fs.writeFileSync(outputFilePath, lines.join('\n'), { encoding: 'utf8' });
  }

  return { schema, counts: { cached, generated, total: docs.length } };
}

type WriteRootIndexFilesOptions = {
  outputFolder: string;
  configPath: string;
  generations: Record<string, GenerationResult>;
};
async function writeRootIndexFiles({ outputFolder, configPath, generations }: WriteRootIndexFilesOptions) {
  // generate entry according to `config.collections`
  const configModPath = path
    .relative(outputFolder, configPath)
    .replace(/\\/g, '/') // replace windows path separator
    .replace(/\.[jt]s$/i, ''); // remove extension (mjs, cjs, mts, and cts are excluded)

  const definitionTypes = Object.keys(generations);

  // write the index.d.ts file
  let lines: string[] = [
    autogeneratedNote,
    '',
    `import type { BaseDoc } from 'markdownlayer/core';`,
    `import config from '${configModPath}'`,
    '',
    `type Definitions = typeof config.definitions`,
    '',
    ...Object.entries(generations).map(([type, { schema }]) => {
      const converted = schema ? `Definitions['${type}']['schema']['_output']` : undefined;
      return `export type ${generateTypeName(type)} = BaseDoc & { data: ${converted ?? 'any'}; };`;
    }),
    '',
    ...definitionTypes.map((type) => `export declare const ${getDataVariableName(type)}: ${generateTypeName(type)}[];`),
    '',
  ];
  let filePath = path.join(outputFolder, 'index.d.ts');
  fs.writeFileSync(filePath, lines.join('\n'), { encoding: 'utf8' });

  // write the index.mjs file
  lines = [
    autogeneratedNote,
    '',
    ...definitionTypes.map((type) => `import { ${getDataVariableName(type)} } from './${type}/index.mjs';`),
    '',
    `export { ${definitionTypes.map((type) => getDataVariableName(type)).join(', ')} };`,
    '',
    `export const allDocuments = [...${definitionTypes.map((type) => getDataVariableName(type)).join(', ...')}];`,
    '',
  ];
  filePath = path.join(outputFolder, 'index.mjs');
  fs.writeFileSync(filePath, lines.join('\n'), { encoding: 'utf8' });
}

// For some reason, generating toc using mdast-util-toc or even using docusaurus' own toc plugin generates entries from frontmatter.
// Most of the time it is the first entry but sometimes it is not. Removing it results in missing TOC at times.
// So we have to do it manually using regex. Hopefully this is a temporary solution and someone will fix this someday.
// Regex solution from https://yusuf.fyi/posts/contentlayer-table-of-contents

const regXHeader = /\n(?<flag>#{1,6})\s+(?<content>.+)/g;

export function generateToc(contents: string): TocItem[] {
  return Array.from(contents.matchAll(regXHeader)).map(({ groups }) => {
    const { flag, content } = groups!;
    // Note: using `slug` instead of `new Slugger()` means no slug deduping.
    const id = githubSlug(content);
    return {
      level: flag.length,
      value: content,
      id: id,
      url: `#${id}`,
    };
  });
}
