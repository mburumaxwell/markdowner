import chokidar from 'chokidar';
import { globby } from 'globby';
import matter from 'gray-matter';
import { existsSync } from 'node:fs';
import { mkdir, readFile, stat, writeFile } from 'node:fs/promises';
import { dirname, join, normalize, relative } from 'node:path';

import { version } from '../package.json';
import { assets } from './assets';
import { getConfig } from './config';
import { MarkdownlayerError, MarkdownlayerErrorData, getYamlErrorLine } from './errors';
import { outputAssets } from './output';
import { resolveSchema, type ResolveSchemaOptions } from './schemas/resolve';
import type {
  DocumentDefinition,
  GenerationMode,
  MarkdownlayerCache,
  MarkdownlayerConfigPlugins,
  ResolvedConfig,
} from './types';
import { generateTypeName, getDataVariableName, idToFileName, makeVariableName } from './utils';

type GeneratedCount = { cached: number; generated: number; total: number };

// NOTE Type assert statements for `.json` files are necessary from Node v16.14 onwards
const nodeVersionMajor = parseInt(process.versions.node.split('.')[0]);
const nodeVersionMinor = parseInt(process.versions.node.split('.')[1]);
const needsJsonAssertStatement = nodeVersionMajor > 16 || (nodeVersionMajor === 16 && nodeVersionMinor >= 14);
const assertStatement = needsJsonAssertStatement ? ` assert { type: 'json' }` : '';
const autogeneratedNote = `// NOTE This file is auto-generated on build.`;

export type GenerateOptions = {
  /**
   * Build mode.
   * Enables production optimizations or development hints.
   */
  mode: GenerationMode;

  /**
   * The path to the configuration file.
   */
  configPath?: string;
};

export async function generate({ mode, configPath: providedConfigPath }: GenerateOptions) {
  // get the config
  const { configPath, configImports, output, contentDirPath, ...config } = await getConfig(providedConfigPath);

  // create output directories if not exists
  await mkdir(output.assets, { recursive: true });

  // generate the content (initial)
  const cwd = process.cwd();
  const outputFolder = join(cwd, '.markdownlayer');
  await generateInner({ mode, outputFolder, output, configPath, contentDirPath, ...config });

  // watch for changes in the config or content folder (development mode only)
  if (mode === 'development' && configPath) {
    const files = [contentDirPath];
    files.push(...configImports); // watch config file and its dependencies

    const watcher = chokidar.watch(files, {
      cwd: contentDirPath,
      ignored: /(^|[/\\])[._]./, // ignore dot & underscore files
      ignoreInitial: true, // ignore initial scan
    });
    watcher.on('all', async (eventName, filename) => {
      if (eventName === 'addDir' || eventName === 'unlinkDir') return; // ignore dir changes
      if (eventName === 'add') console.log(`${filename} added`);
      else if (eventName === 'change') console.log(`${filename} changed`);
      else if (eventName === 'unlink') console.log(`${filename} deleted.`);
      if (filename == null) return;

      filename = join(contentDirPath, filename);

      // remove changed file cache
      for (const [key, value] of Object.entries(config.cache.uniques)) {
        if (value === filename) delete config.cache.uniques[key];
      }

      // changes in the config file should restart the whole process
      if (configImports.includes(filename)) {
        console.log('markdownlayer config changed, restarting...');
        watcher?.close();
        return generate({ mode, configPath: providedConfigPath });
      }

      // regenerate the content
      await generateInner({ mode, outputFolder, configPath, output, contentDirPath, ...config });
    });
  }
}

type GenerateInnerOptions = { mode: GenerationMode; outputFolder: string } & Omit<ResolvedConfig, 'configImports'>;

async function generateInner(options: GenerateInnerOptions) {
  const {
    mode,
    configPath,
    configHash,

    caching = true,
    contentDirPath,
    patterns,
    definitions,
    output,
    mdAsMarkdoc,
    cache: { uniques },
    ...plugins
  } = options;
  let outputFolder = options.outputFolder;

  // load cache from file if it exists, otherwise create a new cache
  // changes in mode, version, configuration options, plugins will invalidate the cache
  const cacheFilePath = join(outputFolder, `cache/${mode}/v${version}/data-${configHash}.json`);
  let cache: MarkdownlayerCache = { uniques, data: { items: {} } };
  if (caching && existsSync(cacheFilePath)) {
    cache = { uniques, data: JSON.parse(await readFile(cacheFilePath, 'utf8')) };
  }

  // iterate over the definitions and generate the docs
  outputFolder = join(outputFolder, 'generated');
  const generations: Record<string, GeneratedCount> = {};
  for (const [type, def] of Object.entries(definitions)) {
    const generation = await generateDocuments({
      ...def,
      type,
      contentDirPath,
      patterns,
      outputFolder,
      mdAsMarkdoc,
      plugins,
      cache,
      output,
    });
    generations[type] = generation;
  }

  // write cache to file
  if (caching) {
    await mkdir(dirname(cacheFilePath), { recursive: true });
    await writeFile(cacheFilePath, JSON.stringify(cache.data, null, 2), { encoding: 'utf8' });
  }

  // write files that would be imported by the application (index.d.ts, index.mjs)
  await writeRootIndexFiles({ mode, outputFolder, configPath, types: Object.keys(generations) });

  // output all assets
  await outputAssets({ destination: output.assets, assets });

  // print some stats
  const { cached, total }: GeneratedCount = Object.values(generations).reduce((acc, count) => {
    acc.cached += count.cached;
    acc.generated += count.generated;
    acc.total += count.total;
    return acc;
  });
  console.log(`Generated ${total} documents (${cached} from cache) in .markdownlayer`);
}

type GenerateDocsOptions = DocumentDefinition & {
  type: string;
  patterns?: string | readonly string[];
  outputFolder: string;
  plugins: MarkdownlayerConfigPlugins;
} & Pick<ResolvedConfig, 'contentDirPath' | 'mdAsMarkdoc' | 'cache' | 'output'>;

async function generateDocuments(options: GenerateDocsOptions): Promise<GeneratedCount> {
  const {
    type,
    format,
    contentDirPath,
    patterns = '**/*.{md,mdoc,mdx}',
    outputFolder,
    mdAsMarkdoc,
    plugins,
    cache,
    output,
  } = options;

  // ensure that all definitions have at least one pattern
  if (patterns.length === 0) {
    throw new MarkdownlayerError(MarkdownlayerErrorData.ConfigNoPatternsError);
  }

  // find the files
  const definitionDir = join(contentDirPath, type);
  const files = await globby(patterns, {
    cwd: definitionDir,
    gitignore: true, // use .gitignore
    ignore: ['**/_*'], // ignore files starting with underscore
    dot: false, // ignore dot files
    onlyFiles: true, // only files, skip directories
  });

  let cached = 0;
  let generated = 0;
  const docs: Record<string, unknown> = {}; // key is document identifier

  await mkdir(join(outputFolder, type), { recursive: true });

  // parse the files and "compile" in a loop
  for (const file of files) {
    const sourceFilePath = normalize(join(definitionDir, file));
    const id = normalize(file);

    // using the sourceFilePath as the key ensure no collisions for files named the same but in different directories;
    const cacheEntryKey = sourceFilePath;

    // if the file has not been modified, use the cached version
    const hash = (await stat(sourceFilePath)).mtimeMs.toString();
    const cacheEntry = cache.data.items[cacheEntryKey];
    const changed = !cacheEntry || cacheEntry.hash !== hash;
    if (!changed) {
      docs[id] = cacheEntry.document;
      cached++;
      continue;
    }

    const contents = await readFile(sourceFilePath, 'utf8');
    const parsedMatter = matter(contents);
    const frontmatter = parsedMatter.data as Record<string, unknown>;

    let data: Record<string, unknown> = frontmatter;
    const resolveSchemaOptions: ResolveSchemaOptions = {
      type,
      schema: options.schema,
      output,
      relativePath: file,
      path: sourceFilePath,
      contents,

      format,
      frontmatter,
      mdAsMarkdoc,
      cache,
      plugins,
    };
    const schema = resolveSchema(resolveSchemaOptions);
    if (schema) {
      const parsed = await schema.safeParseAsync(frontmatter); // Use `safeParseAsync` to allow async transforms
      if (parsed.success) {
        data = parsed.data as Record<string, unknown>;
      } else {
        throw new MarkdownlayerError({
          ...MarkdownlayerErrorData.InvalidDocumentFrontmatterError,
          message: MarkdownlayerErrorData.InvalidDocumentFrontmatterError.message({
            definition: type,
            relativePath: file,
            error: parsed.error,
          }),
          location: {
            file: sourceFilePath,
            line: getYamlErrorLine(parsedMatter.matter, String(parsed.error.errors[0].path[0])),
            column: 0,
          },
        });
      }
    }

    // write json file
    const outputFilePath = join(outputFolder, type, `${idToFileName(file)}.json`);
    await writeFile(outputFilePath, JSON.stringify(data, null, 2), { encoding: 'utf8' });

    // update the cache
    docs[id] = data;
    cache.data.items[cacheEntryKey] = { hash, type, document: data };
    generated++;
  }

  // write the collection file
  let outputFilePath = join(outputFolder, type, 'index.json');
  await writeFile(outputFilePath, JSON.stringify(Object.values(docs), null, 2), { encoding: 'utf8' });

  // write import file
  outputFilePath = join(outputFolder, type, 'index.mjs');
  const lines: string[] = [
    autogeneratedNote,
    '',
    ...Object.keys(docs).map(
      (id) => `import ${makeVariableName(id)} from './${idToFileName(id)}.json'${assertStatement};`,
    ),
    '',
    `export const ${getDataVariableName(type)} = [${Object.keys(docs)
      .map((id) => `${makeVariableName(id)}`)
      .join(', ')}]`,
    '',
  ];
  await writeFile(outputFilePath, lines.join('\n'), { encoding: 'utf8' });

  return { cached, generated, total: Object.keys(docs).length };
}

type WriteRootIndexFilesOptions = {
  mode: GenerationMode;
  outputFolder: string;
  configPath: string;
  types: string[];
};
async function writeRootIndexFiles({ mode, outputFolder, configPath, types }: WriteRootIndexFilesOptions) {
  // generate entry according to `config.collections`
  const configModPath = relative(outputFolder, configPath)
    .replace(/\\/g, '/') // replace windows path separator
    .replace(/\.[jt]s$/i, ''); // remove extension (mjs, cjs, mts, and cts are excluded)

  // write the index.d.ts file
  let lines: string[] = [
    autogeneratedNote,
    '',
    `import { z } from 'zod';`,
    `import config from '${configModPath}'`,
    '',
    'type Definitions = typeof config.definitions;',
    'type ReturnTypeOrOriginal<T> = T extends (...args: any[]) => infer R ? R : T;',
    `type InferSchema<D extends keyof Definitions> = z.infer<ReturnTypeOrOriginal<Definitions[D]['schema']>>;`,
    '',
    ...types.map((type) => `export type ${generateTypeName(type)} = InferSchema<'${type}'>`),
    '',
    ...types.map((type) => `export declare const ${getDataVariableName(type)}: ${generateTypeName(type)}[];`),
    '',
  ];
  let filePath = join(outputFolder, 'index.d.ts');
  await writeFile(filePath, lines.join('\n'), { encoding: 'utf8' });

  // write the index.mjs file
  lines = [
    autogeneratedNote,
    '',
    '// NOTE During development imports are done from `.mjs` files to improve HMR speeds.',
    '// During (production) builds imports are done from `.json` files to improve build performance.',
    '',
    ...types.map((type) => {
      const dataVariableName = getDataVariableName(type);
      return mode == 'development'
        ? `import { ${dataVariableName} } from './${type}/index.mjs';`
        : `import ${dataVariableName} from './${type}/index.json'${assertStatement};`;
    }),
    '',
    `export { ${types.map((type) => getDataVariableName(type)).join(', ')} };`,
    '',
    `export const allDocuments = [...${types.map((type) => getDataVariableName(type)).join(', ...')}];`,
    '',
  ];
  filePath = join(outputFolder, 'index.mjs');
  await writeFile(filePath, lines.join('\n'), { encoding: 'utf8' });
}
